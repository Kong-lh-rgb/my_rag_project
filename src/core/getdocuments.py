import json
from pymilvus import connections, utility, Collection, FieldSchema, CollectionSchema, DataType
from sentence_transformers import SentenceTransformer
from tqdm import tqdm  # å¯¼å…¥ tqdm

# --- 1. è®¾ç½®è¿æ¥å’Œæ¨¡å‹ ---
print("--- 1. è®¾ç½® Milvus è¿æ¥å’Œæ¨¡å‹ ---")
MILVUS_HOST = 'localhost'
MILVUS_PORT = '19530'

try:
    connections.connect(alias="default", host=MILVUS_HOST, port=MILVUS_PORT)
    print("æˆåŠŸè¿æ¥åˆ° Milvus!")
except Exception as e:
    print(f"è¿æ¥å¤±è´¥ï¼š{e}")
    exit()

local_path_bge_m3 = 'D:/Models/bge-m3'
local_path_gte_large_zh = 'D:/Models/gte-large-zh'

print("æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹...")
text_model = SentenceTransformer(local_path_bge_m3)
code_model = SentenceTransformer(local_path_gte_large_zh)
print("æ¨¡å‹åŠ è½½å®Œæˆã€‚")

# --- 2. åŠ è½½åˆ‡åˆ†å¥½çš„æ–‡æ¡£æ•°æ® ---
print("\n--- 2. åŠ è½½åˆ‡åˆ†å¥½çš„æ–‡æ¡£æ•°æ® ---")
try:
    with open(r'D:\pyproject\PythonProject3\data\processed\chunks.json', 'r', encoding='utf-8') as f:
        child_chunks = json.load(f)
    print("æˆåŠŸåŠ è½½ chunks.json æ–‡ä»¶ã€‚")
except FileNotFoundError:
    print("é”™è¯¯ï¼šæœªæ‰¾åˆ° chunks.json æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ prepare_documents.pyã€‚")
    exit()

# --- 3. å®šä¹‰ Milvus Collection Schema ---
print("\n--- 3. æ­£åœ¨åˆ›å»º Milvus Collection ---")
COLLECTION_NAME = "rag_documents"
DIMENSION = 1024

fields = [
    FieldSchema(name="pk", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),
    FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION),
    FieldSchema(name="parent_id", dtype=DataType.VARCHAR, max_length=100),
    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=4096)
]
schema = CollectionSchema(fields, "æ–‡æ¡£å‘é‡ç”¨äºRAGæ£€ç´¢")

if utility.has_collection(COLLECTION_NAME):
    utility.drop_collection(COLLECTION_NAME)

collection = Collection(name=COLLECTION_NAME, schema=schema)

# --- 4. å‘é‡åŒ–å¹¶æ’å…¥æ•°æ® (å·²ä¼˜åŒ–) ---
print("\n--- 4. æ­£åœ¨å‘é‡åŒ–å¹¶æ’å…¥æ•°æ®åˆ° Milvus ---")
BATCH_SIZE = 500  # å®šä¹‰æ¯æ¬¡å¤„ç†çš„æ‰¹æ¬¡å¤§å°

total_chunks = len(child_chunks)
num_batches = (total_chunks + BATCH_SIZE - 1) // BATCH_SIZE

with tqdm(total=total_chunks, desc="å‘é‡åŒ–è¿›åº¦") as pbar:
    for i in range(0, total_chunks, BATCH_SIZE):
        batch_chunks = child_chunks[i:i + BATCH_SIZE]
        data_to_insert = []

        for doc in batch_chunks:
            content = doc["content"]
            doc_type = doc["type"]

            if doc_type == "text":
                print("æ­£åœ¨å¯¹ä¸€ä¸ªæ–‡æœ¬å—è¿›è¡Œå‘é‡åŒ–...")
                vector = text_model.encode(content)
            elif doc_type == "code":
                vector = code_model.encode(content)
            else:
                continue

            data_to_insert.append({
                "pk": doc["id"],
                "vector": vector,
                "parent_id": doc["parent_id"],
                "content": content
            })

        # æ’å…¥å½“å‰æ‰¹æ¬¡çš„æ•°æ®
        collection.insert(data_to_insert)

        # æ›´æ–°è¿›åº¦æ¡
        pbar.update(len(batch_chunks))

print("âœ… æ‰€æœ‰æ•°æ®å·²åˆ†æ‰¹æ¬¡å‘é‡åŒ–å¹¶æ’å…¥æˆåŠŸï¼")

# åˆ›å»ºç´¢å¼•
print("\n--- 5. æ­£åœ¨åˆ›å»ºå‘é‡ç´¢å¼• ---")
index_params = {
    "metric_type": "L2",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 1024}
}
collection.create_index(field_name="vector", index_params=index_params)
collection.load()
print("ğŸ‰ ç´¢å¼•åˆ›å»ºå¹¶åŠ è½½å®Œæˆã€‚æ‰€æœ‰æ–‡æ¡£å·²å‡†å¤‡å°±ç»ªï¼")